{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652dcaa1",
   "metadata": {},
   "source": [
    "# TIME SERIES FORECASTING AND SENTIMENT ANALYSIS OF BIG DATA PROCESSED WITH SparkSQL VS HBASE, CASSANDRA, MONGODB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485163c",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataste is a large dataset gleaned from the twitter API that is called ProjectTweets.csv.\n",
    "\n",
    "This dataset contains 1,600,000 tweets extracted using the twitter api. \n",
    "\n",
    "\n",
    "Content\n",
    "It contains the following 5 fields:\n",
    "- ids: The id of the tweet (eg. 4587)\n",
    "- date: the date of the tweet (eg. Sat May 16 23:58:44 UTC 2009)\n",
    "- flag: The query (eg. lyx). If there is no query, then this value is NO_QUERY.\n",
    "- user: the user that tweeted (eg. bobthebuilder)\n",
    "- text: the text of the tweet (eg. Lyx is cool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bef0b9",
   "metadata": {},
   "source": [
    "# STOP ALL ACTIVE SPARK SESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0164af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/29 18:05:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2ce3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b856db",
   "metadata": {},
   "source": [
    "# Loading Data from local machine to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63a853",
   "metadata": {},
   "source": [
    "One can read  CSV file into DataFrame from hadoop\n",
    "\n",
    "csv_path = \"hdfs://localhost:9000/ProjectTweets/ProjectTweets.csv\"\n",
    "\n",
    "df = spark.read.csv(csv_path, header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a037c60",
   "metadata": {},
   "source": [
    "# Step One: Populating the ProjectTweets CSV Into SparkSQL\n",
    "\n",
    "This involves adding or inserting the ProjectTweets CSV into SparkSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2505d",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Spark Session and Read the ProjectTweets CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a6bee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRIMARY KEY: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "|PRIMARY KEY|        ID|               date|    flag|           user|                text|\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "|          0|1467810369|2009-04-07 06:19:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|          1|1467810672|2009-04-07 06:19:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|          2|1467810917|2009-04-07 06:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|          3|1467811184|2009-04-07 06:19:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|          4|1467811193|2009-04-07 06:19:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark Session\n",
    "spark = SparkSession.builder.appName(\"ProjectTweets CSV to SparkSQL\").getOrCreate()\n",
    "\n",
    "# Set legacy timeParserPolicy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Define the schema for the csv file \n",
    "schema = StructType().add(\"_c0\", StringType(), True).add(\"_c1\", StringType(), True).add(\"_c2\", StringType(), True).add(\"_c3\", StringType(), True).add(\"_c4\", StringType(), True).add(\"_c5\", StringType(), True)\n",
    "\n",
    "# Read the CSV into a DataFrame called df\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"false\").schema(schema).load(\"file:///home/hduser/ProjectTweets.csv\")\n",
    "\n",
    "# Rename the headers\n",
    "df = df.withColumnRenamed(\"_c0\", \"PRIMARY KEY\").withColumnRenamed(\"_c1\", \"ID\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "\n",
    "# Convert string date to TimestampType\n",
    "df = df.withColumn(\"date\", to_timestamp(df[\"date\"], \"EEE MMM dd HH:mm:ss zzzz yyyy\"))\n",
    "\n",
    "# Print schema\n",
    "df.printSchema()\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272db695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRIMARY KEY: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View if the columns have been renamed\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60b8ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "|PRIMARY KEY|        ID|               date|    flag|           user|                text|\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "|          0|1467810369|2009-04-07 06:19:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|          1|1467810672|2009-04-07 06:19:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|          2|1467810917|2009-04-07 06:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|          3|1467811184|2009-04-07 06:19:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|          4|1467811193|2009-04-07 06:19:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View the Spark DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed263d",
   "metadata": {},
   "source": [
    "# Step Two: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b5b96",
   "metadata": {},
   "source": [
    "## Checking for Duplicates (based on ID, user and text) and Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603f5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum  # Import the 'col' and 'sum' functions\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Duplicate and Missing Data Check\").getOrCreate()\n",
    "\n",
    "# Define the columns to check for duplicates and missing values\n",
    "columns_to_check = ['PRIMARY KEY','ID', 'user', 'text']\n",
    "\n",
    "# Create a pipeline to check for duplicates and missing values\n",
    "pipeline_df = df\n",
    "\n",
    "# Step 1: Remove duplicate records based on specified columns\n",
    "pipeline_df = pipeline_df.dropDuplicates(subset=columns_to_check)\n",
    "\n",
    "# Step 2: Check for missing values\n",
    "missing_counts = pipeline_df.select([col(c).isNull().cast(\"int\").alias(c) for c in columns_to_check]).agg(*[sum(c).alias(c) for c in columns_to_check]).collect()[0]\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of duplicate records removed:\", df.count() - pipeline_df.count())\n",
    "\n",
    "print(\"Missing value counts:\")\n",
    "for col_name, missing_count in zip(columns_to_check, missing_counts):\n",
    "    print(col_name, missing_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates in ID and user name\n",
    "# Count the number of rows before removing duplicates\n",
    "count_before = df.count()\n",
    "\n",
    "# Remove duplicates\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "\n",
    "# Count the number of rows after removing duplicates\n",
    "count_after = df_no_duplicates.count()\n",
    "\n",
    "# Calculate the number of duplicates\n",
    "num_duplicates = count_before - count_after\n",
    "\n",
    "print(f\"Number of duplicate rows removed: {num_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5151453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows before removing duplicates\n",
    "count_before = df.count()\n",
    "\n",
    "# Remove duplicates based on a specific column\n",
    "df_no_duplicates = df.dropDuplicates(subset=['user'])\n",
    "\n",
    "# Count the number of rows after removing duplicates\n",
    "count_after = df_no_duplicates.count()\n",
    "\n",
    "# Calculate the number of duplicates\n",
    "num_duplicates = count_before - count_after\n",
    "\n",
    "print(f\"Number of duplicate rows removed based on user: {num_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb892",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438dc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff75fd8",
   "metadata": {},
   "source": [
    "## EXTRACTING TIME COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "\n",
    "\n",
    "#Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DateTime Visualization\").getOrCreate()\n",
    "\n",
    "# Extract relevant time components including hours, minutes, and seconds\n",
    "df = df.withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\")).withColumn(\"day\", dayofmonth(\"date\")).withColumn(\"hour\", hour(\"date\")).withColumn(\"minute\", minute(\"date\")).withColumn(\"second\", second(\"date\"))\n",
    "\n",
    "# Aggregate data\n",
    "time_series_data = df.groupBy(\"date\",\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\").count().orderBy(\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the df DataFrame after extracting time components\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the year variable and count the occurrences\n",
    "year_counts = df.groupBy(\"year\").agg(count(\"*\").alias(\"count\")).orderBy(\"year\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "year_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the month variable and count the occurrences\n",
    "month_counts = df.groupBy(\"month\").agg(count(\"*\").alias(\"count\")).orderBy(\"month\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "month_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0168775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the day variable and count the occurrences\n",
    "day_counts = df.groupBy(\"day\").agg(count(\"*\").alias(\"count\")).orderBy(\"day\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "day_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d960fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the day variable and count the occurrences\n",
    "ID_counts = df.groupBy(\"ID\").agg(count(\"*\").alias(\"count\")).orderBy(\"ID\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "ID_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Text Analysis\").getOrCreate()\n",
    "\n",
    "# Tokenize text using a regular expression tokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=r'\\W')\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=1, vocabSize=1000)\n",
    "model = cv.fit(df)\n",
    "df = model.transform(df)\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a UDF to analyze sentiment using VADER\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = sid.polarity_scores(text)\n",
    "    if sentiment['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Apply the UDF to analyze sentiment for each comment\n",
    "df = df.withColumn(\"sentiment\", sentiment_udf(df[\"text\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.select(\"text\", \"date\", \"sentiment\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f9604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ccd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc454769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, date_format\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Sentiment Forecast Dashboard\").getOrCreate()\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "analyze_sentiment_udf = udf(lambda text: sid.polarity_scores(text)['compound'], StringType())\n",
    "df = df.withColumn(\"sentiment_score\", analyze_sentiment_udf(df[\"text\"]))\n",
    "df = df.withColumn(\"date_str\", date_format(\"date\", \"yyyy-MM-dd\"))\n",
    "df_agg = df.groupBy(\"date_str\").agg({\"sentiment_score\": \"mean\"}).withColumnRenamed(\"avg(sentiment_score)\", \"sentiment_score\")\n",
    "\n",
    "# Step 2: Train a time series forecasting model\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "df_pd = df_agg.toPandas()\n",
    "df_pd['date'] = pd.to_datetime(df_pd['date_str'])\n",
    "\n",
    "# Step 3: Make forecasts\n",
    "# Assuming ARIMA model for simplicity\n",
    "model = ARIMA(df_pd['sentiment_score'], order=(1,1,1))\n",
    "fit_model = model.fit()\n",
    "forecast_1_day = fit_model.forecast(steps=1)\n",
    "forecast_3_days = fit_model.forecast(steps=3)\n",
    "forecast_7_days = fit_model.forecast(steps=7)\n",
    "\n",
    "# Step 4: Display the forecasts (using Plotly for visualization)\n",
    "\n",
    "# Plot historical sentiment data\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_pd['date'], y=df_pd['sentiment_score'], mode='lines', name='Historical Sentiment'))\n",
    "\n",
    "# Plot forecasted values\n",
    "forecast_dates = [df_pd['date'].iloc[-1] + pd.Timedelta(days=i) for i in range(1, 8)]\n",
    "fig.add_trace(go.Scatter(x=forecast_dates, y=forecast_1_day, mode='lines', name='Forecast (1 day)'))\n",
    "fig.add_trace(go.Scatter(x=forecast_dates[:3], y=forecast_3_days, mode='lines', name='Forecast (3 days)'))\n",
    "fig.add_trace(go.Scatter(x=forecast_dates[:7], y=forecast_7_days, mode='lines', name='Forecast (7 days)'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Sentiment Forecast Dashboard',\n",
    "                  xaxis_title='Date',\n",
    "                  yaxis_title='Sentiment Score')\n",
    "\n",
    "# Show the dashboard\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot historical sentiment data\n",
    "fig_historical = go.Figure()\n",
    "fig_historical.add_trace(go.Scatter(x=df_pd['date'], y=df_pd['sentiment_score'], mode='lines', name='Historical Sentiment'))\n",
    "fig_historical.update_layout(title='Historical Sentiment',\n",
    "                             xaxis_title='Date',\n",
    "                             yaxis_title='Sentiment Score')\n",
    "\n",
    "# Plot forecasted values for 1 day\n",
    "fig_1_day = go.Figure()\n",
    "fig_1_day.add_trace(go.Scatter(x=forecast_dates, y=forecast_1_day, mode='lines', name='Forecast (1 day)'))\n",
    "fig_1_day.update_layout(title='Sentiment Forecast (1 day)',\n",
    "                        xaxis_title='Date',\n",
    "                        yaxis_title='Sentiment Score')\n",
    "\n",
    "# Plot forecasted values for 3 days\n",
    "fig_3_days = go.Figure()\n",
    "fig_3_days.add_trace(go.Scatter(x=forecast_dates[:3], y=forecast_3_days, mode='lines', name='Forecast (3 days)'))\n",
    "fig_3_days.update_layout(title='Sentiment Forecast (3 days)',\n",
    "                         xaxis_title='Date',\n",
    "                         yaxis_title='Sentiment Score')\n",
    "\n",
    "# Plot forecasted values for 7 days\n",
    "fig_7_days = go.Figure()\n",
    "fig_7_days.add_trace(go.Scatter(x=forecast_dates[:7], y=forecast_7_days, mode='lines', name='Forecast (7 days)'))\n",
    "fig_7_days.update_layout(title='Sentiment Forecast (7 days)',\n",
    "                         xaxis_title='Date',\n",
    "                         yaxis_title='Sentiment Score')\n",
    "\n",
    "# Show the separate figures\n",
    "fig_historical.show()\n",
    "fig_1_day.show()\n",
    "fig_3_days.show()\n",
    "fig_7_days.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, date_format\n",
    "from pyspark.sql.types import StringType\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Sentiment Forecast Dashboard\").getOrCreate()\n",
    "\n",
    "# Assuming df is your Spark DataFrame containing the sentiment data\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "analyze_sentiment_udf = udf(lambda text: sid.polarity_scores(text)['compound'], StringType())\n",
    "df = df.withColumn(\"sentiment_score\", analyze_sentiment_udf(df[\"text\"]))\n",
    "df = df.withColumn(\"month\", date_format(\"date\", \"MM\"))\n",
    "df = df.withColumn(\"day\", date_format(\"date\", \"dd\"))\n",
    "df_agg = df.groupBy(\"month\", \"day\").agg({\"sentiment_score\": \"mean\"}).withColumnRenamed(\"avg(sentiment_score)\", \"sentiment_score\")\n",
    "\n",
    "# Step 2: Train a time series forecasting model\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "df_pd = df_agg.toPandas()\n",
    "df_pd['date'] = pd.to_datetime(df_pd['month'] + '-' + df_pd['day'])\n",
    "\n",
    "# Step 3: Make forecasts\n",
    "# Assuming ARIMA model for simplicity\n",
    "model = ARIMA(df_pd['sentiment_score'], order=(1,1,1))\n",
    "fit_model = model.fit()\n",
    "forecast_1_day = fit_model.forecast(steps=1)\n",
    "forecast_3_days = fit_model.forecast(steps=3)\n",
    "forecast_7_days = fit_model.forecast(steps=7)\n",
    "\n",
    "# Step 4: Display the forecasts (using Plotly for visualization)\n",
    "\n",
    "# Create a dropdown menu for selecting months\n",
    "month_buttons = [\n",
    "    {'label': f'Month {month}', 'method': 'update', 'args': [{'visible': [df_pd['month'] == month]}]}\n",
    "    for month in df_pd['month'].unique()\n",
    "]\n",
    "\n",
    "# Plot historical sentiment data\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_pd['day'], y=df_pd['sentiment_score'], mode='lines', name='Historical Sentiment'))\n",
    "\n",
    "# Plot forecasted values\n",
    "forecast_dates = [df_pd['date'].iloc[-1] + pd.Timedelta(days=i) for i in range(1, 8)]\n",
    "fig.add_trace(go.Scatter(x=forecast_dates, y=forecast_1_day, mode='lines', name='Forecast (1 day)'))\n",
    "fig.add_trace(go.Scatter(x=forecast_dates[:3], y=forecast_3_days, mode='lines', name='Forecast (3 days)'))\n",
    "fig.add_trace(go.Scatter(x=forecast_dates[:7], y=forecast_7_days, mode='lines', name='Forecast (7 days)'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Sentiment Forecast Dashboard',\n",
    "                  xaxis_title='Day',\n",
    "                  yaxis_title='Sentiment Score',\n",
    "                  updatemenus=[{'buttons': month_buttons, 'direction': 'down', 'showactive': True, 'x': 0.5, 'xanchor': 'center'}],\n",
    "                  showlegend=True)\n",
    "\n",
    "# Show the dashboard\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae440bf7",
   "metadata": {},
   "source": [
    "# Populated the Project Tweets CSV  into the NoSQL DATABASE (MONGODB) USING SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Process data from MongoDB using spark\").config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/project.ProjectTweets\").config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/Project.my_output_ProjectTweets\").getOrCreate()\n",
    "\n",
    "# Read data from MongoDB into a DataFrame\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Spark transformations and actions on the DataFrame\n",
    "# For example:\n",
    "df_processed = df.select(\"field1\", \"field2\").filter(df[\"field1\"] > 10)\n",
    "\n",
    "# Write processed data back to MongoDB\n",
    "df_processed.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
