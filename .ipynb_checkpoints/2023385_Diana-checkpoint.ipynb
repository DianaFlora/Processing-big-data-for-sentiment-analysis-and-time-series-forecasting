{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a691eb-491c-4cbd-acfd-363055534da4",
   "metadata": {},
   "source": [
    "# TIME SERIES FORECASTING OF PROJECT TWEETS BIG DATA PROCESSED WITH SPARK AND STORED IN MONGODB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf4c2b-41fe-4f80-a139-066374224315",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataste is a large dataset gleaned from the twitter API that is called ProjectTweets.csv.\n",
    "\n",
    "This dataset contains 1,600,000 tweets extracted using the twitter api. \n",
    "\n",
    "\n",
    "Content\n",
    "It contains the following 5 fields:\n",
    "- ids: The id of the tweet (eg. 4587)\n",
    "- date: the date of the tweet (eg. Sat May 16 23:58:44 UTC 2009)\n",
    "- flag: The query (eg. lyx). If there is no query, then this value is NO_QUERY.\n",
    "- user: the user that tweeted (eg. bobthebuilder)\n",
    "- text: the text of the tweet (eg. Lyx is cool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c402ab-f73e-44fa-9bcb-3cd3686a1d6e",
   "metadata": {},
   "source": [
    "## Install all Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a43f76-594a-4b56-88e2-fa2bb26f45cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, trim, split, udf\n",
    "from pyspark.sql.functions import isnull, to_timestamp\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # We can suppress the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7dfddb-3db1-4417-8904-25e89ff1daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pyspark session connecting to mongodb\n",
    "uri = \"mongodb://172.17.0.8:27017/Diana_Project_mongo.Tweets\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Write into MongoDB\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", uri)\\\n",
    "    .config(\"spark.mongodb.output.uri\", uri)\\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:2.4.2')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b77330-d443-4ce3-b6c4-2dd938b1284c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4dde10a4171c:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Write into MongoDB</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff4c177990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f184ca6-02a1-49d4-9e34-39b94abba76b",
   "metadata": {},
   "source": [
    "# Step one: Writing Data into MongoDB using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31398c42-136d-4ff4-b665-0932e06a61f7",
   "metadata": {},
   "source": [
    "## Loading data from local machine to SParkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55462fc4-56f4-4222-a185-b00572045103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRIMARY KEY: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "|PRIMARY KEY|        ID|               date|    flag|           user|                text|\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "|          0|1467810369|2009-04-07 05:19:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|          1|1467810672|2009-04-07 05:19:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|          2|1467810917|2009-04-07 05:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|          3|1467811184|2009-04-07 05:19:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|          4|1467811193|2009-04-07 05:19:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set legacy timeParserPolicy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Define the schema for the csv file \n",
    "schema = StructType().add(\"_c0\", StringType(), True).add(\"_c1\", StringType(), True).add(\"_c2\", StringType(), True).add(\"_c3\", StringType(), True).add(\"_c4\", StringType(), True).add(\"_c5\", StringType(), True)\n",
    "\n",
    "# Read the CSV into a DataFrame called df\n",
    "df = spark.read.format(\"csv\").option(\"header\", False).schema(schema).load(\"file:///home/jovyan/Diana/ProjectTweets.csv\")\n",
    "\n",
    "# Rename the headers\n",
    "df = df.withColumnRenamed(\"_c0\", \"PRIMARY KEY\").withColumnRenamed(\"_c1\", \"ID\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "\n",
    "# Convert string date to TimestampType\n",
    "df = df.withColumn(\"date\", to_timestamp(df[\"date\"], \"EEE MMM dd HH:mm:ss zzzz yyyy\"))\n",
    "\n",
    "# Print schema\n",
    "df.printSchema()\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566df067-a314-47b2-a4a4-ca03159f58c0",
   "metadata": {},
   "source": [
    "## Write data from spark to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25ea7c1-ebd1-4d4a-88d9-a03a091d224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"uri\", uri).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1058c99-28db-47dd-9276-f51419737644",
   "metadata": {},
   "source": [
    "# Step Two: Read the Project Tweets data from MongoDB using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e81c22-f29d-45c9-98bf-f2dcc2585c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 7)\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- PRIMARY KEY: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+\n",
      "|        ID|PRIMARY KEY|                 _id|               date|    flag|                text|           user|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+\n",
      "|1551363506|     816210|{66364d27fdf41a68...|2009-04-18 15:51:40|NO_QUERY|@ctribe I hope yo...|  prosario_2000|\n",
      "|1551363569|     816211|{66364d27fdf41a68...|2009-04-18 15:51:39|NO_QUERY|@Boy_Kill_Boy Nop...|Chelsea_Volturi|\n",
      "|1551363682|     816212|{66364d27fdf41a68...|2009-04-18 15:51:41|NO_QUERY|@marty0518 Someti...|askbillmitchell|\n",
      "|1551363752|     816213|{66364d27fdf41a68...|2009-04-18 15:51:41|NO_QUERY|so i guesss im no...|       kendiixd|\n",
      "|1551363844|     816214|{66364d27fdf41a68...|2009-04-18 15:51:42|NO_QUERY|@DaiLS I do that,...|    ladycalypso|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from_mongo = spark.read.format('com.mongodb.spark.sql.DefaultSource').load()\n",
    "print((from_mongo.count(), len(from_mongo.columns)))\n",
    "from_mongo.printSchema()\n",
    "from_mongo.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e3bdf-eed2-442b-815b-bf2375d0c84d",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbf0a7-9366-41bc-9a1b-bff77cf5d2db",
   "metadata": {},
   "source": [
    "# Checking for Duplicates (based on ID, user and text) and Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "187c1a83-e90c-4eff-8310-967d2c6c62b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records removed: 0\n",
      "Missing value counts:\n",
      "PRIMARY KEY 0\n",
      "ID 0\n",
      "user 0\n",
      "text 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum  # Import the 'col' and 'sum' functions\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Duplicate and Missing Data Check\").getOrCreate()\n",
    "\n",
    "# Define the columns to check for duplicates and missing values\n",
    "columns_to_check = ['PRIMARY KEY','ID', 'user', 'text']\n",
    "\n",
    "# Create a pipeline to check for duplicates and missing values\n",
    "pipeline_df = from_mongo\n",
    "\n",
    "# Step 1: Remove duplicate records based on specified columns\n",
    "pipeline_df = pipeline_df.dropDuplicates(subset=columns_to_check)\n",
    "\n",
    "# Step 2: Check for missing values\n",
    "missing_counts = pipeline_df.select([col(c).isNull().cast(\"int\").alias(c) for c in columns_to_check]).agg(*[sum(c).alias(c) for c in columns_to_check]).collect()[0]\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of duplicate records removed:\", df.count() - pipeline_df.count())\n",
    "\n",
    "print(\"Missing value counts:\")\n",
    "for col_name, missing_count in zip(columns_to_check, missing_counts):\n",
    "    print(col_name, missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed96930b-c1ab-4e9a-bad7-0ba5b186de0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "#Checking for duplicates in ID and user name\n",
    "# Count the number of rows before removing duplicates\n",
    "count_before = from_mongo.count()\n",
    "\n",
    "# Remove duplicates\n",
    "df_no_duplicates = from_mongo.dropDuplicates()\n",
    "\n",
    "# Count the number of rows after removing duplicates\n",
    "count_after = df_no_duplicates.count()\n",
    "\n",
    "# Calculate the number of duplicates\n",
    "num_duplicates = count_before - count_after\n",
    "\n",
    "print(f\"Number of duplicate rows removed: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a932d2b7-a0b9-484a-8374-24873e591aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows removed based on user: 940225\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows before removing duplicates\n",
    "count_before = from_mongo.count()\n",
    "\n",
    "# Remove duplicates based on a specific column\n",
    "df_no_duplicates = from_mongo.dropDuplicates(subset=['user'])\n",
    "\n",
    "# Count the number of rows after removing duplicates\n",
    "count_after = df_no_duplicates.count()\n",
    "\n",
    "# Calculate the number of duplicates\n",
    "num_duplicates = count_before - count_after\n",
    "\n",
    "print(f\"Number of duplicate rows removed based on user: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c5cba-92d8-401f-b90a-a9d692beaef9",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ede2aa7-cdb7-4e5e-b464-2ec6a84f41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+--------+--------------------+--------------------+\n",
      "|summary|                  ID|      PRIMARY KEY|    flag|                text|                user|\n",
      "+-------+--------------------+-----------------+--------+--------------------+--------------------+\n",
      "|  count|             1600000|          1600000| 1600000|             1600000|             1600000|\n",
      "|   mean|1.9988175522956276E9|         799999.5|    NULL|                NULL| 4.325887521835714E9|\n",
      "| stddev| 1.935760736226746E8|461880.3596892475|    NULL|                NULL|5.162733218454887...|\n",
      "|    min|          1467810369|                0|NO_QUERY|                 ...|        000catnap000|\n",
      "|    max|          2329205794|           999999|NO_QUERY|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|          zzzzeus111|\n",
      "+-------+--------------------+-----------------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summary Statistics\n",
    "from_mongo.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434c55c-c286-4dc3-808d-2238fe7857aa",
   "metadata": {},
   "source": [
    "# EXTRACTING TIME COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe21863a-1658-407e-911a-1fda5e8dde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "\n",
    "\n",
    "#Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DateTime Visualization\").getOrCreate()\n",
    "\n",
    "# Extract relevant time components including hours, minutes, and seconds\n",
    "df = from_mongo.withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\")).withColumn(\"day\", dayofmonth(\"date\")).withColumn(\"hour\", hour(\"date\")).withColumn(\"minute\", minute(\"date\")).withColumn(\"second\", second(\"date\"))\n",
    "\n",
    "# Aggregate data\n",
    "time_series_data = df.groupBy(\"date\",\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\").count().orderBy(\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6abfb12d-a4e9-42f9-a369-0da22a65198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "|        ID|PRIMARY KEY|                 _id|               date|    flag|                text|           user|year|month|day|hour|minute|second|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "|1551363506|     816210|{66364d27fdf41a68...|2009-04-18 15:51:40|NO_QUERY|@ctribe I hope yo...|  prosario_2000|2009|    4| 18|  15|    51|    40|\n",
      "|1551363569|     816211|{66364d27fdf41a68...|2009-04-18 15:51:39|NO_QUERY|@Boy_Kill_Boy Nop...|Chelsea_Volturi|2009|    4| 18|  15|    51|    39|\n",
      "|1551363682|     816212|{66364d27fdf41a68...|2009-04-18 15:51:41|NO_QUERY|@marty0518 Someti...|askbillmitchell|2009|    4| 18|  15|    51|    41|\n",
      "|1551363752|     816213|{66364d27fdf41a68...|2009-04-18 15:51:41|NO_QUERY|so i guesss im no...|       kendiixd|2009|    4| 18|  15|    51|    41|\n",
      "|1551363844|     816214|{66364d27fdf41a68...|2009-04-18 15:51:42|NO_QUERY|@DaiLS I do that,...|    ladycalypso|2009|    4| 18|  15|    51|    42|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View the df DataFrame after extracting time components\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5ddeaa-d79e-4072-bc25-b5a199762d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- PRIMARY KEY: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- second: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bdc1db1-372d-4a9c-b0d6-f349f16c28d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|year|  count|\n",
      "+----+-------+\n",
      "|2009|1600000|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the year variable and count the occurrences\n",
    "year_counts = df.groupBy(\"year\").agg(count(\"*\").alias(\"count\")).orderBy(\"year\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "year_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fd43d47-9889-4b8a-bd4c-0d21769af583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|month| count|\n",
      "+-----+------+\n",
      "|    4|100025|\n",
      "|    5|559073|\n",
      "|    6|940902|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the month variable and count the occurrences\n",
    "month_counts = df.groupBy(\"month\").agg(count(\"*\").alias(\"count\")).orderBy(\"month\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "month_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf0b81c-185e-4e34-9015-bfa8daf9ff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|day| count|\n",
      "+---+------+\n",
      "|  1| 95449|\n",
      "|  2|108872|\n",
      "|  3| 86707|\n",
      "|  4| 32938|\n",
      "|  5| 34735|\n",
      "|  6|104793|\n",
      "|  7|132564|\n",
      "|  8| 18566|\n",
      "| 10| 31551|\n",
      "| 11|  6217|\n",
      "| 12|  4186|\n",
      "| 14| 22026|\n",
      "| 15| 83309|\n",
      "| 16| 87524|\n",
      "| 17| 85236|\n",
      "| 18|105040|\n",
      "| 19| 75612|\n",
      "| 20| 64029|\n",
      "| 21| 41782|\n",
      "| 22| 49519|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the day variable and count the occurrences\n",
    "day_counts = df.groupBy(\"day\").agg(count(\"*\").alias(\"count\")).orderBy(\"day\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "day_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b299e87-f152-4d96-8ccb-4e2d086352a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "|month|day| count|\n",
      "+-----+---+------+\n",
      "|    4|  7| 20671|\n",
      "|    4| 18| 17154|\n",
      "|    4| 20| 18447|\n",
      "|    4| 21| 11105|\n",
      "|    4| 19| 32648|\n",
      "|    5|  4| 28300|\n",
      "|    5|  2| 31096|\n",
      "|    5|  3| 26568|\n",
      "|    5| 22| 41206|\n",
      "|    5| 24|   169|\n",
      "|    5| 17| 41205|\n",
      "|    5| 12|  4186|\n",
      "|    5| 10| 31551|\n",
      "|    5| 29| 60227|\n",
      "|    5| 18| 44564|\n",
      "|    5| 27| 11619|\n",
      "|    5| 11|  6217|\n",
      "|    5| 14| 21526|\n",
      "|    5| 25|   169|\n",
      "|    5| 30|104484|\n",
      "+-----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the day variable and count the occurrences\n",
    "day_counts = df.groupBy(\"month\",\"day\").agg(count(\"*\").alias(\"count\")).orderBy(\"month\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "day_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "541597f2-d4a9-4ad7-926d-ac1e9143e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|        ID|count|\n",
      "+----------+-----+\n",
      "|1467810369|    1|\n",
      "|1467810672|    1|\n",
      "|1467810917|    1|\n",
      "|1467811184|    1|\n",
      "|1467811193|    1|\n",
      "|1467811372|    1|\n",
      "|1467811592|    1|\n",
      "|1467811594|    1|\n",
      "|1467811795|    1|\n",
      "|1467812025|    1|\n",
      "|1467812416|    1|\n",
      "|1467812579|    1|\n",
      "|1467812723|    1|\n",
      "|1467812771|    1|\n",
      "|1467812784|    1|\n",
      "|1467812799|    1|\n",
      "|1467812964|    1|\n",
      "|1467813137|    1|\n",
      "|1467813579|    1|\n",
      "|1467813782|    1|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the day variable and count the occurrences\n",
    "ID_counts = df.groupBy(\"ID\").agg(count(\"*\").alias(\"count\")).orderBy(\"ID\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "ID_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa8b6aa-5236-4a88-a720-4bf30e7efac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+------+-----+\n",
      "|               date|year|month|day|hour|minute|second|count|\n",
      "+-------------------+----+-----+---+----+------+------+-----+\n",
      "|2009-04-07 05:19:45|2009|    4|  7|   5|    19|    45|    1|\n",
      "|2009-04-07 05:19:49|2009|    4|  7|   5|    19|    49|    1|\n",
      "|2009-04-07 05:19:53|2009|    4|  7|   5|    19|    53|    1|\n",
      "|2009-04-07 05:19:57|2009|    4|  7|   5|    19|    57|    2|\n",
      "|2009-04-07 05:20:00|2009|    4|  7|   5|    20|     0|    1|\n",
      "+-------------------+----+-----+---+----+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_series_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e106b132-eff5-43bf-a5d7-8d8d313cff17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b7a2cf6-988e-4f0d-8c95-b9e8af71d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "|        ID|PRIMARY KEY|                 _id|               date|    flag|                text|           user|year|month|day|hour|minute|second|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "|1551363506|     816210|{66364d27fdf41a68...|2009-04-18 15:51:40|NO_QUERY|@ctribe I hope yo...|  prosario_2000|2009|    4| 18|  15|    51|    40|\n",
      "|1551363569|     816211|{66364d27fdf41a68...|2009-04-18 15:51:39|NO_QUERY|@Boy_Kill_Boy Nop...|Chelsea_Volturi|2009|    4| 18|  15|    51|    39|\n",
      "|1551363682|     816212|{66364d27fdf41a68...|2009-04-18 15:51:41|NO_QUERY|@marty0518 Someti...|askbillmitchell|2009|    4| 18|  15|    51|    41|\n",
      "|1551363752|     816213|{66364d27fdf41a68...|2009-04-18 15:51:41|NO_QUERY|so i guesss im no...|       kendiixd|2009|    4| 18|  15|    51|    41|\n",
      "|1551363844|     816214|{66364d27fdf41a68...|2009-04-18 15:51:42|NO_QUERY|@DaiLS I do that,...|    ladycalypso|2009|    4| 18|  15|    51|    42|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5363f5b-5885-4097-8cb6-05ee4d0bea80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c454c80-e4e2-4dc9-aa0a-24c70f313ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the identifiers\n",
    "df = df.drop(\"_id\", \"ID\", \"PRIMARY KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00fec948-c900-4408-81a4-1c31dd2017d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the df spark DataFrame as a csv\n",
    "df.write.csv(\"tweeter_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0755814-c467-431c-8ebf-abf68c40938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the Sp to JSON format\n",
    "df.write.json(\"tweeterdata.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
