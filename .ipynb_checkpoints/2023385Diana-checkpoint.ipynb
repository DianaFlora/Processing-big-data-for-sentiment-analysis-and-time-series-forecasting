{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a691eb-491c-4cbd-acfd-363055534da4",
   "metadata": {},
   "source": [
    "# TIME SERIES FORECASTING OF PROJECT TWEETS BIG DATA PROCESSED WITH SPARK AND STORED IN MONGODB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf4c2b-41fe-4f80-a139-066374224315",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataste is a large dataset gleaned from the twitter API that is called ProjectTweets.csv.\n",
    "\n",
    "This dataset contains 1,600,000 tweets extracted using the twitter api. \n",
    "\n",
    "\n",
    "Content\n",
    "It contains the following 5 fields:\n",
    "- ids: The id of the tweet (eg. 4587)\n",
    "- date: the date of the tweet (eg. Sat May 16 23:58:44 UTC 2009)\n",
    "- flag: The query (eg. lyx). If there is no query, then this value is NO_QUERY.\n",
    "- user: the user that tweeted (eg. bobthebuilder)\n",
    "- text: the text of the tweet (eg. Lyx is cool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c402ab-f73e-44fa-9bcb-3cd3686a1d6e",
   "metadata": {},
   "source": [
    "## Install all Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a43f76-594a-4b56-88e2-fa2bb26f45cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, trim, split, udf\n",
    "from pyspark.sql.functions import isnull, to_timestamp\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # We can suppress the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7dfddb-3db1-4417-8904-25e89ff1daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a pyspark session connecting to mongodb\n",
    "uri = \"mongodb://172.17.0.8:27017/DeeProject_mongo.Tweets\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Write into MongoDB\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", uri)\\\n",
    "    .config(\"spark.mongodb.output.uri\", uri)\\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:2.4.2')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b77330-d443-4ce3-b6c4-2dd938b1284c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4dde10a4171c:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Write into MongoDB</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff7a1e4bd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Spark content\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f184ca6-02a1-49d4-9e34-39b94abba76b",
   "metadata": {},
   "source": [
    "# Step one: Writing Data into MongoDB using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31398c42-136d-4ff4-b665-0932e06a61f7",
   "metadata": {},
   "source": [
    "## Loading data from local machine to SParkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55462fc4-56f4-4222-a185-b00572045103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRIMARY KEY: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "|PRIMARY KEY|        ID|               date|    flag|           user|                text|\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "|          0|1467810369|2009-04-07 05:19:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|          1|1467810672|2009-04-07 05:19:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|          2|1467810917|2009-04-07 05:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|          3|1467811184|2009-04-07 05:19:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|          4|1467811193|2009-04-07 05:19:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----------+----------+-------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set legacy timeParserPolicy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "#Define the schema for the csv file \n",
    "schema = StructType().add(\"_c0\", StringType(), True).add(\"_c1\", StringType(), True).add(\"_c2\", StringType(), True).add(\"_c3\", StringType(), True).add(\"_c4\", StringType(), True).add(\"_c5\", StringType(), True)\n",
    "\n",
    "#Read the CSV into a DataFrame called df\n",
    "df = spark.read.format(\"csv\").option(\"header\", False).schema(schema).load(\"file:///home/jovyan/Diana/ProjectTweets.csv\")\n",
    "\n",
    "#Rename the headers\n",
    "df = df.withColumnRenamed(\"_c0\", \"PRIMARY KEY\").withColumnRenamed(\"_c1\", \"ID\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "\n",
    "#Convert string date to TimestampType\n",
    "df = df.withColumn(\"date\", to_timestamp(df[\"date\"], \"EEE MMM dd HH:mm:ss zzzz yyyy\"))\n",
    "\n",
    "#Print schema\n",
    "df.printSchema()\n",
    "\n",
    "#Show DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566df067-a314-47b2-a4a4-ca03159f58c0",
   "metadata": {},
   "source": [
    "## Write data from spark to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25ea7c1-ebd1-4d4a-88d9-a03a091d224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write data into MongoDB\n",
    "df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"uri\", uri).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1058c99-28db-47dd-9276-f51419737644",
   "metadata": {},
   "source": [
    "# Step Two: Read the Project Tweets data from MongoDB using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e81c22-f29d-45c9-98bf-f2dcc2585c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 7)\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- PRIMARY KEY: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+\n",
      "|        ID|PRIMARY KEY|                 _id|               date|    flag|                text|         user|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+\n",
      "|1551363506|     816210|{66367bb0e048fa3c...|2009-04-18 15:51:40|NO_QUERY|@ctribe I hope yo...|prosario_2000|\n",
      "|2059493951|     408810|{66367bb0e048fa3c...|2009-06-07 00:02:45|NO_QUERY|Kinda scared to s...|        l7l7v|\n",
      "|1990436550|    1223636|{66367bb0e048fa3c...|2009-06-01 11:52:03|NO_QUERY|@karinhoegh  Didn...|         kmdk|\n",
      "|1990436582|    1223637|{66367bb0e048fa3c...|2009-06-01 11:52:03|NO_QUERY|Need more FPS.......| jflinchbaugh|\n",
      "|1990436614|    1223638|{66367bb0e048fa3c...|2009-06-01 11:52:04|NO_QUERY|@SteveOGallagher ...|    kittaykat|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read Data from MongoDB\n",
    "from_mongo = spark.read.format('com.mongodb.spark.sql.DefaultSource').load()\n",
    "print((from_mongo.count(), len(from_mongo.columns)))\n",
    "from_mongo.printSchema()\n",
    "from_mongo.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e3bdf-eed2-442b-815b-bf2375d0c84d",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbf0a7-9366-41bc-9a1b-bff77cf5d2db",
   "metadata": {},
   "source": [
    "# Checking for Duplicates (based on ID, user and text) and Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "187c1a83-e90c-4eff-8310-967d2c6c62b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records removed: 0\n",
      "Missing value counts:\n",
      "PRIMARY KEY 0\n",
      "ID 0\n",
      "user 0\n",
      "text 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum  # Import the 'col' and 'sum' functions\n",
    "\n",
    "#Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Duplicate and Missing Data Check\").getOrCreate()\n",
    "\n",
    "#Define the columns to check for duplicates and missing values\n",
    "columns_to_check = ['PRIMARY KEY','ID', 'user', 'text']\n",
    "\n",
    "#Create a pipeline to check for duplicates and missing values\n",
    "pipeline_df = from_mongo\n",
    "\n",
    "#Step 1: Remove duplicate records based on specified columns\n",
    "pipeline_df = pipeline_df.dropDuplicates(subset=columns_to_check)\n",
    "\n",
    "#Step 2: Check for missing values\n",
    "missing_counts = pipeline_df.select([col(c).isNull().cast(\"int\").alias(c) for c in columns_to_check]).agg(*[sum(c).alias(c) for c in columns_to_check]).collect()[0]\n",
    "\n",
    "#Print the results\n",
    "print(\"Number of duplicate records removed:\", df.count() - pipeline_df.count())\n",
    "\n",
    "print(\"Missing value counts:\")\n",
    "for col_name, missing_count in zip(columns_to_check, missing_counts):\n",
    "    print(col_name, missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed96930b-c1ab-4e9a-bad7-0ba5b186de0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "#Checking for duplicates in ID and user name\n",
    "#Count the number of rows before removing duplicates\n",
    "count_before = from_mongo.count()\n",
    "\n",
    "#Remove duplicates\n",
    "df_no_duplicates = from_mongo.dropDuplicates()\n",
    "\n",
    "#Count the number of rows after removing duplicates\n",
    "count_after = df_no_duplicates.count()\n",
    "\n",
    "#Calculate the number of duplicates\n",
    "num_duplicates = count_before - count_after\n",
    "\n",
    "print(f\"Number of duplicate rows removed: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a932d2b7-a0b9-484a-8374-24873e591aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows removed based on user: 940225\n"
     ]
    }
   ],
   "source": [
    "#Count the number of rows before removing duplicates\n",
    "count_before = from_mongo.count()\n",
    "\n",
    "#Remove duplicates based on a specific column\n",
    "df_no_duplicates = from_mongo.dropDuplicates(subset=['user'])\n",
    "\n",
    "#Count the number of rows after removing duplicates\n",
    "count_after = df_no_duplicates.count()\n",
    "\n",
    "#Calculate the number of duplicates\n",
    "num_duplicates = count_before - count_after\n",
    "\n",
    "print(f\"Number of duplicate rows removed based on user: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c5cba-92d8-401f-b90a-a9d692beaef9",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ede2aa7-cdb7-4e5e-b464-2ec6a84f41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+--------+--------------------+--------------------+\n",
      "|summary|                  ID|       PRIMARY KEY|    flag|                text|                user|\n",
      "+-------+--------------------+------------------+--------+--------------------+--------------------+\n",
      "|  count|             1600000|           1600000| 1600000|             1600000|             1600000|\n",
      "|   mean|1.9988175522956276E9|          799999.5|    NULL|                NULL| 4.325887521835714E9|\n",
      "| stddev|1.9357607362267783E8|461880.35968924506|    NULL|                NULL|5.162733218454889E10|\n",
      "|    min|          1467810369|                 0|NO_QUERY|                 ...|        000catnap000|\n",
      "|    max|          2329205794|            999999|NO_QUERY|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|          zzzzeus111|\n",
      "+-------+--------------------+------------------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summary Statistics\n",
    "from_mongo.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434c55c-c286-4dc3-808d-2238fe7857aa",
   "metadata": {},
   "source": [
    "# EXTRACTING TIME COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe21863a-1658-407e-911a-1fda5e8dde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "\n",
    "\n",
    "#Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DateTime Visualization\").getOrCreate()\n",
    "\n",
    "#Extract relevant time components including hours, minutes, and seconds\n",
    "df = from_mongo.withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\")).withColumn(\"day\", dayofmonth(\"date\")).withColumn(\"hour\", hour(\"date\")).withColumn(\"minute\", minute(\"date\")).withColumn(\"second\", second(\"date\"))\n",
    "\n",
    "#Aggregate data\n",
    "time_series_data = df.groupBy(\"date\",\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\").count().orderBy(\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6abfb12d-a4e9-42f9-a369-0da22a65198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "|        ID|PRIMARY KEY|                 _id|               date|    flag|                text|         user|year|month|day|hour|minute|second|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "|1551363506|     816210|{66367bb0e048fa3c...|2009-04-18 15:51:40|NO_QUERY|@ctribe I hope yo...|prosario_2000|2009|    4| 18|  15|    51|    40|\n",
      "|2059493951|     408810|{66367bb0e048fa3c...|2009-06-07 00:02:45|NO_QUERY|Kinda scared to s...|        l7l7v|2009|    6|  7|   0|     2|    45|\n",
      "|1990436550|    1223636|{66367bb0e048fa3c...|2009-06-01 11:52:03|NO_QUERY|@karinhoegh  Didn...|         kmdk|2009|    6|  1|  11|    52|     3|\n",
      "|1990436582|    1223637|{66367bb0e048fa3c...|2009-06-01 11:52:03|NO_QUERY|Need more FPS.......| jflinchbaugh|2009|    6|  1|  11|    52|     3|\n",
      "|1990436614|    1223638|{66367bb0e048fa3c...|2009-06-01 11:52:04|NO_QUERY|@SteveOGallagher ...|    kittaykat|2009|    6|  1|  11|    52|     4|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View the df DataFrame after extracting time components\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5ddeaa-d79e-4072-bc25-b5a199762d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- PRIMARY KEY: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- second: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View the Spark DataFrame Features\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bdc1db1-372d-4a9c-b0d6-f349f16c28d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|year|  count|\n",
      "+----+-------+\n",
      "|2009|1600000|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "#Group by the year variable and count the occurrences\n",
    "year_counts = df.groupBy(\"year\").agg(count(\"*\").alias(\"count\")).orderBy(\"year\")\n",
    "\n",
    "#Show the tabulated counts\n",
    "year_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fd43d47-9889-4b8a-bd4c-0d21769af583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|month| count|\n",
      "+-----+------+\n",
      "|    4|100025|\n",
      "|    5|559073|\n",
      "|    6|940902|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "#Group by the month variable and count the occurrences\n",
    "month_counts = df.groupBy(\"month\").agg(count(\"*\").alias(\"count\")).orderBy(\"month\")\n",
    "\n",
    "#Show the tabulated counts\n",
    "month_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf0b81c-185e-4e34-9015-bfa8daf9ff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|day| count|\n",
      "+---+------+\n",
      "|  1| 95449|\n",
      "|  2|108872|\n",
      "|  3| 86707|\n",
      "|  4| 32938|\n",
      "|  5| 34735|\n",
      "|  6|104793|\n",
      "|  7|132564|\n",
      "|  8| 18566|\n",
      "| 10| 31551|\n",
      "| 11|  6217|\n",
      "| 12|  4186|\n",
      "| 14| 22026|\n",
      "| 15| 83309|\n",
      "| 16| 87524|\n",
      "| 17| 85236|\n",
      "| 18|105040|\n",
      "| 19| 75612|\n",
      "| 20| 64029|\n",
      "| 21| 41782|\n",
      "| 22| 49519|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "#Group by the day variable and count the occurrences\n",
    "day_counts = df.groupBy(\"day\").agg(count(\"*\").alias(\"count\")).orderBy(\"day\")\n",
    "\n",
    "#Show the tabulated counts\n",
    "day_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b299e87-f152-4d96-8ccb-4e2d086352a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "|month|day| count|\n",
      "+-----+---+------+\n",
      "|    4|  7| 20671|\n",
      "|    4| 18| 17154|\n",
      "|    4| 20| 18447|\n",
      "|    4| 21| 11105|\n",
      "|    4| 19| 32648|\n",
      "|    5|  4| 28300|\n",
      "|    5| 10| 31551|\n",
      "|    5|  2| 31096|\n",
      "|    5|  3| 26568|\n",
      "|    5| 22| 41206|\n",
      "|    5| 24|   169|\n",
      "|    5| 17| 41205|\n",
      "|    5| 12|  4186|\n",
      "|    5| 29| 60227|\n",
      "|    5| 18| 44564|\n",
      "|    5| 27| 11619|\n",
      "|    5| 11|  6217|\n",
      "|    5| 14| 21526|\n",
      "|    5| 25|   169|\n",
      "|    5| 30|104484|\n",
      "+-----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the day variable and count the occurrences\n",
    "day_counts = df.groupBy(\"month\",\"day\").agg(count(\"*\").alias(\"count\")).orderBy(\"month\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "day_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "541597f2-d4a9-4ad7-926d-ac1e9143e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|        ID|count|\n",
      "+----------+-----+\n",
      "|1467810369|    1|\n",
      "|1467810672|    1|\n",
      "|1467810917|    1|\n",
      "|1467811184|    1|\n",
      "|1467811193|    1|\n",
      "|1467811372|    1|\n",
      "|1467811592|    1|\n",
      "|1467811594|    1|\n",
      "|1467811795|    1|\n",
      "|1467812025|    1|\n",
      "|1467812416|    1|\n",
      "|1467812579|    1|\n",
      "|1467812723|    1|\n",
      "|1467812771|    1|\n",
      "|1467812784|    1|\n",
      "|1467812799|    1|\n",
      "|1467812964|    1|\n",
      "|1467813137|    1|\n",
      "|1467813579|    1|\n",
      "|1467813782|    1|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the day variable and count the occurrences\n",
    "ID_counts = df.groupBy(\"ID\").agg(count(\"*\").alias(\"count\")).orderBy(\"ID\")\n",
    "\n",
    "# Show the tabulated counts\n",
    "ID_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa8b6aa-5236-4a88-a720-4bf30e7efac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+------+-----+\n",
      "|               date|year|month|day|hour|minute|second|count|\n",
      "+-------------------+----+-----+---+----+------+------+-----+\n",
      "|2009-04-07 05:19:45|2009|    4|  7|   5|    19|    45|    1|\n",
      "|2009-04-07 05:19:49|2009|    4|  7|   5|    19|    49|    1|\n",
      "|2009-04-07 05:19:53|2009|    4|  7|   5|    19|    53|    1|\n",
      "|2009-04-07 05:19:57|2009|    4|  7|   5|    19|    57|    2|\n",
      "|2009-04-07 05:20:00|2009|    4|  7|   5|    20|     0|    1|\n",
      "+-------------------+----+-----+---+----+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_series_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e106b132-eff5-43bf-a5d7-8d8d313cff17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b7a2cf6-988e-4f0d-8c95-b9e8af71d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "|        ID|PRIMARY KEY|                 _id|               date|    flag|                text|         user|year|month|day|hour|minute|second|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "|1551363506|     816210|{66367bb0e048fa3c...|2009-04-18 15:51:40|NO_QUERY|@ctribe I hope yo...|prosario_2000|2009|    4| 18|  15|    51|    40|\n",
      "|2059493951|     408810|{66367bb0e048fa3c...|2009-06-07 00:02:45|NO_QUERY|Kinda scared to s...|        l7l7v|2009|    6|  7|   0|     2|    45|\n",
      "|1990436550|    1223636|{66367bb0e048fa3c...|2009-06-01 11:52:03|NO_QUERY|@karinhoegh  Didn...|         kmdk|2009|    6|  1|  11|    52|     3|\n",
      "|1990436582|    1223637|{66367bb0e048fa3c...|2009-06-01 11:52:03|NO_QUERY|Need more FPS.......| jflinchbaugh|2009|    6|  1|  11|    52|     3|\n",
      "|1990436614|    1223638|{66367bb0e048fa3c...|2009-06-01 11:52:04|NO_QUERY|@SteveOGallagher ...|    kittaykat|2009|    6|  1|  11|    52|     4|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5363f5b-5885-4097-8cb6-05ee4d0bea80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "888f4542-a27e-4089-b039-42b85b778464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "|        ID|PRIMARY KEY|                 _id|               date|    flag|                text|         user|year|month|day|hour|minute|second|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "|1551363506|     816210|{66367bb0e048fa3c...|2009-04-18 15:51:40|NO_QUERY|@ctribe I hope yo...|prosario_2000|2009|    4| 18|  15|    51|    40|\n",
      "|2059493951|     408810|{66367bb0e048fa3c...|2009-06-07 00:02:45|NO_QUERY|Kinda scared to s...|        l7l7v|2009|    6|  7|   0|     2|    45|\n",
      "|1990436550|    1223636|{66367bb0e048fa3c...|2009-06-01 11:52:03|NO_QUERY|@karinhoegh  Didn...|         kmdk|2009|    6|  1|  11|    52|     3|\n",
      "|1990436582|    1223637|{66367bb0e048fa3c...|2009-06-01 11:52:03|NO_QUERY|Need more FPS.......| jflinchbaugh|2009|    6|  1|  11|    52|     3|\n",
      "|1990436614|    1223638|{66367bb0e048fa3c...|2009-06-01 11:52:04|NO_QUERY|@SteveOGallagher ...|    kittaykat|2009|    6|  1|  11|    52|     4|\n",
      "+----------+-----------+--------------------+-------------------+--------+--------------------+-------------+----+-----+---+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c454c80-e4e2-4dc9-aa0a-24c70f313ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"_id\", \"ID\", \"PRIMARY KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31d4a827-9c5c-40f1-ac43-113524c40842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "|               date|    flag|                text|           user|year|month|day|hour|minute|second|\n",
      "+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "|2009-04-18 15:51:40|NO_QUERY|@ctribe I hope yo...|  prosario_2000|2009|    4| 18|  15|    51|    40|\n",
      "|2009-06-07 00:02:45|NO_QUERY|Kinda scared to s...|          l7l7v|2009|    6|  7|   0|     2|    45|\n",
      "|2009-06-01 11:52:03|NO_QUERY|@karinhoegh  Didn...|           kmdk|2009|    6|  1|  11|    52|     3|\n",
      "|2009-06-01 11:52:03|NO_QUERY|Need more FPS.......|   jflinchbaugh|2009|    6|  1|  11|    52|     3|\n",
      "|2009-06-01 11:52:04|NO_QUERY|@SteveOGallagher ...|      kittaykat|2009|    6|  1|  11|    52|     4|\n",
      "|2009-04-18 15:51:39|NO_QUERY|@Boy_Kill_Boy Nop...|Chelsea_Volturi|2009|    4| 18|  15|    51|    39|\n",
      "|2009-06-01 11:52:04|NO_QUERY|Can't wait for th...|          Mm_Ka|2009|    6|  1|  11|    52|     4|\n",
      "|2009-06-01 11:52:04|NO_QUERY|@nickayre Lol. Bu...|    bryancheung|2009|    6|  1|  11|    52|     4|\n",
      "|2009-06-01 11:52:05|NO_QUERY|I love it when I ...|      videohive|2009|    6|  1|  11|    52|     5|\n",
      "|2009-04-18 15:51:41|NO_QUERY|@marty0518 Someti...|askbillmitchell|2009|    4| 18|  15|    51|    41|\n",
      "|2009-06-01 11:52:05|NO_QUERY|http://twitpic.co...|     kayepintac|2009|    6|  1|  11|    52|     5|\n",
      "|2009-04-18 15:51:41|NO_QUERY|so i guesss im no...|       kendiixd|2009|    4| 18|  15|    51|    41|\n",
      "|2009-06-01 11:52:05|NO_QUERY|@STARBUCK_NOLA Wh...|     Dayewalker|2009|    6|  1|  11|    52|     5|\n",
      "|2009-04-18 15:51:42|NO_QUERY|@DaiLS I do that,...|    ladycalypso|2009|    4| 18|  15|    51|    42|\n",
      "|2009-06-01 11:52:06|NO_QUERY|Taking down the t...|          padrg|2009|    6|  1|  11|    52|     6|\n",
      "|2009-04-18 15:51:43|NO_QUERY|trendy topic - Re...| FindingAnswers|2009|    4| 18|  15|    51|    43|\n",
      "|2009-06-01 11:52:06|NO_QUERY|@Jon_W_Turner i k...| electronichaze|2009|    6|  1|  11|    52|     6|\n",
      "|2009-04-18 15:51:43|NO_QUERY|@firsttiger Real ...|      HTwashere|2009|    4| 18|  15|    51|    43|\n",
      "|2009-06-01 11:52:08|NO_QUERY|@freshleafdesign ...|  runwaycrochet|2009|    6|  1|  11|    52|     8|\n",
      "|2009-04-18 15:51:43|NO_QUERY|@Dragoncade I see...|     kelliekano|2009|    4| 18|  15|    51|    43|\n",
      "+-------------------+--------+--------------------+---------------+----+-----+---+----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adcd709",
   "metadata": {},
   "source": [
    "## Save the df Dataframe as a Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1c79181-cf3f-4a96-b13e-250d46b8f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5f72732-7fb8-4d2b-a717-1089f13a4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#Create pandas DataFrame from the list of rows\n",
    "pandas_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0a3de3-5f9d-460b-9c03-1323dc5d9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.columns = df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79d6ea32-f739-4923-8a77-e21fdb1bca4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00fec948-c900-4408-81a4-1c31dd2017d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file path where you want to save the CSV file\n",
    "file_path = \"pandas_data.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "pandas_df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03cea26",
   "metadata": {},
   "source": [
    "# Save the processed Data to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7aa9eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write data into MongoDB\n",
    "df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"uri\", uri).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f115c9",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS COMPARING VADER VS TEXTBLOB, THEN TIME SERIES FORECASTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366ebf1",
   "metadata": {},
   "source": [
    "## Text/Tweets Processing\n",
    "This includes the following steps:-\n",
    "- Read and Load the Dataset\n",
    "- Exploratory Data Analysis\n",
    "- Data Visualization of Target Variables\n",
    "- Data Preprocessing\n",
    "- Splitting our data into Train and Test sets.\n",
    "- Transforming Dataset using TF-IDF Vectorizer\n",
    "- Function for Model Evaluation\n",
    "- Model Building\n",
    "- Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8399d",
   "metadata": {},
   "source": [
    "# Read the  pandas_data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load the data\n",
    "df = pd.read_csv(\"C:/Users/Diana/Documents/Semester 2/sem two ca 2/pandas_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc43617",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "This process involves:-\n",
    "\n",
    "a) View the first and last few observations of the df dataframe\n",
    "\n",
    "b) View the number of observations and variables the df dataframe has\n",
    "\n",
    "c) View the entire df dataframe to check the data types and any missing data in a particluar variable.\n",
    " \n",
    "d) Checking for missing data/dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the first few observations of the df DataFrame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the last few observations of the df DataFrame\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5967b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the shape of the df DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the features\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date_column' to datetime datatype\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the date has been changed to date time\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6e655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check the df\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f578007",
   "metadata": {},
   "source": [
    "## Drop Variables that will not be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07fb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns except 'date' and 'text'\n",
    "df = df[['date', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if they have been dropped\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'date' column in ascending order\n",
    "df = df.sort_values(by='date', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88867fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the date is sorted\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format if it's not already in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract month from 'date' column\n",
    "df['month'] = df['date'].dt.to_period('M')\n",
    "\n",
    "#Group by month and find minimum and maximum date for each month\n",
    "monthly_date_range = df.groupby('month')['date'].agg([min, max])\n",
    "\n",
    "#Display the result\n",
    "print(monthly_date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330b0fb",
   "metadata": {},
   "source": [
    "## The dates have some missing dates\n",
    "- The data is for 3 months, April, May, June\n",
    "- April (7/4/2009 - 21/4/2009)\n",
    "- May (2/5/2009) - 31/5/2009\n",
    "- June (1/6/2009 -25/6/2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of dates by month\n",
    "monthly_date_counts = df.groupby('month').size()\n",
    "\n",
    "# Display the result\n",
    "print(monthly_date_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df['date'].dt.to_period('D')\n",
    "\n",
    "# Count the number of dates by month\n",
    "dm_date_counts = df.groupby(['month', 'day']).size()\n",
    "\n",
    "# Display the result\n",
    "print(dm_date_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87750dc",
   "metadata": {},
   "source": [
    "There are missing dates in each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc972211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique dates\n",
    "unique_date_counts = df['date'].nunique()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of unique dates:\", unique_date_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65efdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the df\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a6738",
   "metadata": {},
   "source": [
    "# EDA of the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of words in the text\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a83a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"text\",\"word_count\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maximum number of words in the 'word_count' variable of the 'df' DataFrame\n",
    "largest_word_count = df[\"word_count\"].max()\n",
    "largest_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of characters in the text variable\n",
    "df['char_count'] = df['text'].str.len() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad625a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the head of the char_count and text\n",
    "df[[\"text\",\"char_count\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maximum number of characters in the 'char_count' variable of the 'df' DataFrame\n",
    "largest_char_count = df[\"char_count\"].max()\n",
    "largest_char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a43398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to calculate the average length of words in a sentence\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the avg_word function on text\n",
    "df['avg_word'] = df['text'].apply(lambda x: avg_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af70949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View text and avg_word\n",
    "df[['text','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34790040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the highest average word count of the 'df' DataFrame\n",
    "highest_avg_word = df[\"avg_word\"].max()\n",
    "highest_avg_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ecb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3292c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of stopwords in each text and store in a variable called stopwords\n",
    "df['stopwords'] = df['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['text','stopwords']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13457131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maximum number of stopwords in the 'df' DataFrame\n",
    "maximum_no_stopwords = df[\"stopwords\"].max()\n",
    "maximum_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e61fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of hashtags in each text and store in a variable called hastags\n",
    "df['hashtags'] = df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "df[['text','hashtags']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e809f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maximum number of hashtags in the 'df' DataFrame\n",
    "maximum_no_hashtags = df[\"hashtags\"].max()\n",
    "maximum_no_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded05876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of @ signs in text variable and store the value in a variable called at_sign\n",
    "df['at_sign'] = df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
    "df[['text','at_sign']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maximum number of at_sign in the 'df' DataFrame\n",
    "maximum_no_atsign = df[\"at_sign\"].max()\n",
    "maximum_no_atsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147097f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of numerics in the text variable and store the value in a variable called numeric\n",
    "df['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "df[['text','numerics']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bf746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maximum number of numerics in the 'df' DataFrame\n",
    "maximum_no_numerics = df[\"numerics\"].max()\n",
    "maximum_no_numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3766413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of uppercases in the text variable and store the value in a variable called upper\n",
    "df['upper'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "df[['text','upper']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626bd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maximum number of uppercases in the 'df' DataFrame\n",
    "maximum_no_uppercases = df[\"upper\"].max()\n",
    "maximum_no_uppercases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540853d",
   "metadata": {},
   "source": [
    "## Tweets Processing\n",
    "Tweet/text processing involves:-\n",
    "\n",
    "a) Text Normalization \n",
    "- Remove special characters\n",
    "- Change the upper cases to lower cases\n",
    "- Remove numbers/integers\n",
    "- Remove punctuations\n",
    "- Remove white space\n",
    "- Remove URLS/links\n",
    "\n",
    "b) Tokenization\n",
    "- Tokenization\n",
    "\n",
    "c) Remove stopwords\n",
    "\n",
    "d) Stemming/ lemmatization\n",
    "\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19841ca9",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4e26e",
   "metadata": {},
   "source": [
    "### Remove user name from text @username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Define a function to remove user names from text\n",
    "def remove_usernames(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "#Apply the function to the 'text' variable\n",
    "df['text1'] = df['text'].apply(remove_usernames)\n",
    "\n",
    "# Print the first few rows of the DataFrame with user names removed from text\n",
    "print(df['text1'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a8c22e",
   "metadata": {},
   "source": [
    "### Remove url/www.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(df):\n",
    "    #Define regex pattern to match URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    #Apply regex substitution to each row of the 'text' column\n",
    "    df['text1'] = df['text1'].apply(lambda text: re.sub(url_pattern, '', text))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# remove a url\n",
    "df = remove_urls(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6dfdb",
   "metadata": {},
   "source": [
    "### Remove all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac14dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all special characters\n",
    "df['text1'] = df['text1'].str.replace('[^\\w\\s]','')\n",
    "df['text1'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d4d6a",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1880099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "#Apply remove_punctuation function to the 'text' column\n",
    "df['text1'] = df['text1'].apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001622d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b092d",
   "metadata": {},
   "source": [
    "### Convert all upper cases to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all uppercases to lower cases\n",
    "df['text1'] = df['text1'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['text1'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7471e7",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5aa21",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b209b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Tokenize the text using NLTK's word tokenizer\n",
    "df['tokenized_text'] = df['text1'].apply(lambda x: ' '.join(word_tokenize(x.lower())))\n",
    "\n",
    "#Print the first few rows of the DataFrame with tokenized text\n",
    "df[['text1', 'tokenized_text']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a2c8b",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559282bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Download NLTK resources for the first time\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Get the English stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Function to remove stopwords\n",
    "def remove_stopwords(tokenized_text):\n",
    "    return ' '.join([word for word in tokenized_text.split() if word.lower() not in stop_words])\n",
    "\n",
    "#Remove stopwords from the tokenized text column\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(remove_stopwords)\n",
    "\n",
    "# Print the first few rows of the DataFrame with stopwords removed\n",
    "print(df[['text1', 'tokenized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Frequency Distribution of the tokenized words\n",
    "#Import Frequency Distribution\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#Find frequency distribution of tokenized_text\n",
    "fdist = FreqDist(df['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check top 5 common words\n",
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d491a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a frequency distribution plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot Frequency Distribution\n",
    "fdist.plot(20,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228717a",
   "metadata": {},
   "source": [
    "## Stemming/ lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eed33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(tokenized_text):\n",
    "    #Lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokenized_text.split()]\n",
    "    \n",
    "    #Join the lemmatized tokens back into a string\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "#Apply lemmatization to 'tokenized_text' column in DataFrame df\n",
    "df['preprocessed_text'] = df['tokenized_text'].apply(lemmatize_text)\n",
    "\n",
    "#Print the first few rows of the DataFrame with lemmatized text\n",
    "print(df[['tokenized_text', 'preprocessed_text']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb30a3",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS\n",
    "There are various ways to perform sentiment analysis. These include:-\n",
    "- Using Text Blob\n",
    "- Using Vader\n",
    "- Using Bag of Words Vectorization-based Models\n",
    "- Using LSTM-based Models\n",
    "- Using Transformer-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa0312",
   "metadata": {},
   "source": [
    "## USING TEXTBLOB\n",
    "-  It takes text as an input and can return polarity and subjectivity as outputs.\n",
    "\n",
    "- Polarity determines the sentiment of the text. Its values lie in [-1,1] where -1 denotes a highly negative sentiment and 1 denotes a highly positive sentiment.\n",
    "\n",
    "- Subjectivity determines whether a text input is factual information or a personal opinion. Its value lies between [0,1] where a value closer to 0 denotes a piece of factual information and a value closer to 1 denotes a personal opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#Apply sentiment analysis using TextBlob to the 'text' column and storing the polarity score\n",
    "df['polarity_score'] = df['preprocessed_text'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "df[['preprocessed_text','polarity_score']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b357cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the df, first 5 observations\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addca82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View data types of the variables\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize sentiment polarities\n",
    "def categorize_sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply the function to classify polarity scores and store the result in 'textblob_sentiment' column\n",
    "df['textblob_sentiment'] = df['polarity_score'].apply(categorize_sentiment)\n",
    "\n",
    "# Print the first few rows of the DataFrame with 'textblob_sentiment' column\n",
    "print(df[['preprocessed_text', 'polarity_score', 'textblob_sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize sentiment polarities\n",
    "# Categorize sentiment polarities\n",
    "def categorize_sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return 1  # Positive\n",
    "    elif polarity < 0:\n",
    "        return -1  # Negative\n",
    "    else:\n",
    "        return 0  # Neutral\n",
    "\n",
    "# Apply the function to classify polarity scores and store the result in 'textblob_sentiment' column\n",
    "df['textblobsentiment'] = df['polarity_score'].apply(categorize_sentiment)\n",
    "\n",
    "#View the first few rows of the DataFrame with 'textblob_sentiment' column\n",
    "df[['preprocessed_text', 'polarity_score', 'textblob_sentiment','textblobsentiment']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cc4b4",
   "metadata": {},
   "source": [
    "# USING VADER SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a VADER SentimentIntensityAnalyzer instance\n",
    "twitter_sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Create a function to get compound sentiment score using VADER\n",
    "def get_sentiment_scores(text):\n",
    "    return twitter_sentiment.polarity_scores(text)\n",
    "\n",
    "#Apply the function to get the compound scores and store the values in vader_sentiment\n",
    "df['vader_sentiment'] = df['preprocessed_text'].apply(get_sentiment_scores)\n",
    "\n",
    "#Extract the 'neg', 'neu', 'pos', 'compound' scores from 'vader_sentiment'\n",
    "df['vader_neg'] = df['vader_sentiment'].apply(lambda x: x['neg'])\n",
    "df['vader_neu'] = df['vader_sentiment'].apply(lambda x: x['neu'])\n",
    "df['vader_pos'] = df['vader_sentiment'].apply(lambda x: x['pos'])\n",
    "df['vader_compound'] = df['vader_sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "#Create a function to categorize sentiments based on compound scores\n",
    "def categorize_sentiment(vader_compound):\n",
    "    if vader_compound >= 0.05:\n",
    "        return 'positive'\n",
    "    elif vader_compound <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "#Apply the categorization function to create the 'sentiment' column\n",
    "df['sentiment_vader'] = df['vader_compound'].apply(categorize_sentiment)\n",
    "\n",
    "#Display the updated DataFrame with the 'sentiment' column\n",
    "print(df[['preprocessed_text','vader_compound','sentiment_vader']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c258aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View the first 5 observations\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ead0c",
   "metadata": {},
   "source": [
    "# Determine which one to use between vader and textblob by evaluating their performance using "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c07492",
   "metadata": {},
   "source": [
    "# Using TfidfVectorizer, countvectorizer AND MNB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a14a0e",
   "metadata": {},
   "source": [
    "## Textblob sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Vectorize the Text Data using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    "                        stop_words='english', ngram_range=(1,1))\n",
    "X = tfidf.fit_transform(df['text'])\n",
    "y = df['textblob_sentiment']\n",
    "\n",
    "#Split the data into Train-Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create a classifier for MNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "#Train the Classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate the Model\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83241110",
   "metadata": {},
   "source": [
    "## vader sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the Text Data using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    "                        stop_words='english', ngram_range=(1,1))\n",
    "X = tfidf.fit_transform(df['text'])\n",
    "y = df['sentiment_vader']\n",
    "\n",
    "#Split the data into Train-Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create a classifier for MNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "#Train the Classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate the Model\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06692d6a",
   "metadata": {},
   "source": [
    "## USING COUNTVECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv    = CountVectorizer(stop_words = 'english',ngram_range = (1, 1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(df['text'])\n",
    "\n",
    "text_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915952d",
   "metadata": {},
   "source": [
    "## textblob sentiments using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd928b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, df['textblob_sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "#Training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "\n",
    "#Caluclate the accuracy score of the model\n",
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(\"Accuracuy Score: \",accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793e625",
   "metadata": {},
   "source": [
    "## vader sentiments using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, df['sentiment_vader'], test_size=0.25, random_state=5)\n",
    "\n",
    "#Training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "\n",
    "#Caluclate the accuracy score of the model\n",
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(\"Accuracuy Score: \",accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79053494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the variables data type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47448b",
   "metadata": {},
   "source": [
    "Since the accuracy of sentiments extracted by textblob is higher sentiments extracted by textblob will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08d1cb",
   "metadata": {},
   "source": [
    "## Encode the Sentiments\n",
    "Try \n",
    "label and one hot and see which one performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf6e0e",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'sentiment' column\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['textblob_sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffedaa8e",
   "metadata": {},
   "source": [
    "## One-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a OneHotEncoder object\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the 'sentiment' column\n",
    "sentiment_onehot = onehot_encoder.fit_transform(df['textblob_sentiment'].values.reshape(-1, 1))\n",
    "\n",
    "# Convert the one-hot encoded result to a DataFrame\n",
    "sentiment_onehot_df = pd.DataFrame(sentiment_onehot.toarray(), columns=onehot_encoder.categories_[0])\n",
    "\n",
    "# Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "df = pd.concat([df, sentiment_onehot_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9ae35f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:45</th>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:49</th>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:53</th>\n",
       "      <td>dived many time ball managed save 50 rest go b...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:57</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:57</th>\n",
       "      <td>behaving im mad cant see</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     preprocessed_text  \\\n",
       "date                                                                     \n",
       "2009-04-07 05:19:45  awww thats bummer shoulda got david carr third...   \n",
       "2009-04-07 05:19:49  upset cant update facebook texting might cry r...   \n",
       "2009-04-07 05:19:53  dived many time ball managed save 50 rest go b...   \n",
       "2009-04-07 05:19:57                    whole body feel itchy like fire   \n",
       "2009-04-07 05:19:57                           behaving im mad cant see   \n",
       "\n",
       "                     sentiment_encoded textblob_sentiment  \n",
       "date                                                       \n",
       "2009-04-07 05:19:45                  2           Positive  \n",
       "2009-04-07 05:19:49                  1            Neutral  \n",
       "2009-04-07 05:19:53                  2           Positive  \n",
       "2009-04-07 05:19:57                  2           Positive  \n",
       "2009-04-07 05:19:57                  0           Negative  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all columns that are unused\n",
    "df2 = df[['preprocessed_text','sentiment_encoded','textblob_sentiment','sentiment_onehot_df']]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e979e066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:45</th>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:49</th>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:53</th>\n",
       "      <td>dived many time ball managed save 50 rest go b...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:57</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:57</th>\n",
       "      <td>behaving im mad cant see</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     preprocessed_text  \\\n",
       "date                                                                     \n",
       "2009-04-07 05:19:45  awww thats bummer shoulda got david carr third...   \n",
       "2009-04-07 05:19:49  upset cant update facebook texting might cry r...   \n",
       "2009-04-07 05:19:53  dived many time ball managed save 50 rest go b...   \n",
       "2009-04-07 05:19:57                    whole body feel itchy like fire   \n",
       "2009-04-07 05:19:57                           behaving im mad cant see   \n",
       "\n",
       "                     sentiment_encoded textblob_sentiment  \n",
       "date                                                       \n",
       "2009-04-07 05:19:45                  2           Positive  \n",
       "2009-04-07 05:19:49                  1            Neutral  \n",
       "2009-04-07 05:19:53                  2           Positive  \n",
       "2009-04-07 05:19:57                  2           Positive  \n",
       "2009-04-07 05:19:57                  0           Negative  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vie the data info\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "28809602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1600000 entries, 2009-04-07 05:19:45 to 2009-06-25 17:28:31\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count    Dtype \n",
      "---  ------              --------------    ----- \n",
      " 0   preprocessed_text   1600000 non-null  object\n",
      " 1   sentiment_encoded   1600000 non-null  int32 \n",
      " 2   textblob_sentiment  1600000 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 75.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#Check the df2 info\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8fa4d82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:45</th>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:49</th>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:53</th>\n",
       "      <td>dived many time ball managed save 50 rest go b...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:57</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-07 05:19:57</th>\n",
       "      <td>behaving im mad cant see</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     preprocessed_text  \\\n",
       "date                                                                     \n",
       "2009-04-07 05:19:45  awww thats bummer shoulda got david carr third...   \n",
       "2009-04-07 05:19:49  upset cant update facebook texting might cry r...   \n",
       "2009-04-07 05:19:53  dived many time ball managed save 50 rest go b...   \n",
       "2009-04-07 05:19:57                    whole body feel itchy like fire   \n",
       "2009-04-07 05:19:57                           behaving im mad cant see   \n",
       "\n",
       "                     sentiment_encoded textblob_sentiment  \n",
       "date                                                       \n",
       "2009-04-07 05:19:45                  2           Positive  \n",
       "2009-04-07 05:19:49                  1            Neutral  \n",
       "2009-04-07 05:19:53                  2           Positive  \n",
       "2009-04-07 05:19:57                  2           Positive  \n",
       "2009-04-07 05:19:57                  0           Negative  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "866c6d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.220185e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.618375e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment_encoded\n",
       "count       1.600000e+06\n",
       "mean        1.220185e+00\n",
       "std         7.618375e-01\n",
       "min         0.000000e+00\n",
       "25%         1.000000e+00\n",
       "50%         1.000000e+00\n",
       "75%         2.000000e+00\n",
       "max         2.000000e+00"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb9b53",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b7d75685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Counts:\n",
      "Positive    679250\n",
      "Neutral     593796\n",
      "Negative    326954\n",
      "Name: textblob_sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Count occurrences of each sentiment class\n",
    "sentiment_counts = df2['textblob_sentiment'].value_counts()\n",
    "\n",
    "# Display the result as a table\n",
    "print(\"Sentiment Counts:\")\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc1af8",
   "metadata": {},
   "source": [
    "There is class imbalance, in the sentiment data. Class imbalance occurs when one class (or classes) has significantly more samples than the other classes. In this case, the counts of sentiment classes are as follows:\n",
    "\n",
    "\n",
    "Class 2: 679,250 samples\n",
    "\n",
    "Class 0: 593,796 samples\n",
    "\n",
    "Class 1: 326,954 samples\n",
    "    \n",
    "The class imbalance can potentially affect the performance of the machine learning model, especially if the minority class (in this case, class 1) \n",
    "\n",
    "To address class imbalance here are some techniques to be considered:-\n",
    "\n",
    "- Resampling: Either oversampling the minority class (creating more samples of the minority class) or undersampling the majority class (removing samples from the majority class).\n",
    "\n",
    "- Class weights: Assigning higher weights to the minority class during model training to give it more importance.\n",
    "\n",
    "- Synthetic data generation: Generating synthetic samples for the minority class using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "- Different algorithms: Using algorithms that are less sensitive to class imbalance, such as decision trees or random forests.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781ded1",
   "metadata": {},
   "source": [
    "I will explore two options\n",
    "- Resampling\n",
    "- Class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4db8ed",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "Resampling is a technique used to address class imbalance by either oversampling the minority class (creating more samples of the minority class) or undersampling the majority class (removing samples from the majority class). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9dfc7",
   "metadata": {},
   "source": [
    "# Oversampling the minority class (i.e Sentiment = class -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48450fe",
   "metadata": {},
   "source": [
    "# LSTM for sentiment analysis\n",
    "The steps to be considered include:-\n",
    "\n",
    "- Preprocess the text data- i am using already preprocessed text data\n",
    "- Tokenize the preprocessed data\n",
    "- Prepare data for training\n",
    "- split the data into training and test set\n",
    "- define and train the LSTM model\n",
    "- Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d4f262c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4db5de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the preprocessed text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df2['preprocessed_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df2['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e8d81cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 37\n"
     ]
    }
   ],
   "source": [
    "#Find the maximum sequence length\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "print(\"Maximum sequence length:\", max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "73ed5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data for training\n",
    "max_len = 37  # Max sequence length\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "y = df2['sentiment_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b9653125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 9052 1724    2]\n",
      " [   0    0    0 ...   12  179 1069]\n",
      " [   0    0    0 ...  369    6 2984]\n",
      " ...\n",
      " [   0    0    0 ...  954 2414   49]\n",
      " [   0    0    0 ... 1359   36   66]\n",
      " [   0    0    0 ...   99  104 1321]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ac06be1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 37)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1068ad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 ... 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "491dc735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000,)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "40ecfa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280000, 37), (320000, 37), (1280000,), (320000,))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d970a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dtype: int32\n",
      "y_train dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d91568d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_len))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer with 3 units for 3 sentiment classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9cedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 3038/10000 [========>.....................] - ETA: 1:17:39 - loss: 0.1127 - accuracy: 0.9620"
     ]
    }
   ],
   "source": [
    "#Model compilation and training\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b576557b",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.9 GiB for an array with shape (1600000, 3000) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Prepare the data for training\u001b[39;00m\n\u001b[0;32m      9\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3000\u001b[39m  \u001b[38;5;66;03m# Max sequence length\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m X \u001b[38;5;241m=\u001b[39m pad_sequences(sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_len)\n\u001b[0;32m     11\u001b[0m y \u001b[38;5;241m=\u001b[39m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextblob_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Split the data into training and testing sets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:1120\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_str:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dtype` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not compatible with `value`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms type: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou should set `dtype=object` for variable length \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1118\u001b[0m     )\n\u001b[1;32m-> 1120\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((num_samples, maxlen) \u001b[38;5;241m+\u001b[39m sample_shape, value, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences):\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\numeric.py:344\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m    342\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m asarray(fill_value)\n\u001b[0;32m    343\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m--> 344\u001b[0m a \u001b[38;5;241m=\u001b[39m empty(shape, dtype, order)\n\u001b[0;32m    345\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 17.9 GiB for an array with shape (1600000, 3000) and data type int32"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the testing set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab45bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape of train and test df\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11124cc2",
   "metadata": {},
   "source": [
    "# LONG-SHORT-TERM MEMORY (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af84319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52b9a0",
   "metadata": {},
   "source": [
    "## Drop unused variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6de47",
   "metadata": {},
   "source": [
    "# LSTM (Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf98e59",
   "metadata": {},
   "source": [
    "# ONE-HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1db6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b903b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the sentiment_vader variable\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "sentiment_encoded = onehot_encoder.fit_transform(df[['sentiment_vader']])\n",
    "vader_df = pd.concat([df[['date']], pd.DataFrame(sentiment_encoded, columns=onehot_encoder.categories_[0])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a004fc",
   "metadata": {},
   "source": [
    "# Split the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b6326f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280000, 4), (320000, 4))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "467edfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d530f3d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(0, 1, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:144\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(0, 1, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Reshape into X=t and Y=t+1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m look_back \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 3\u001b[0m trainX, trainY \u001b[38;5;241m=\u001b[39m create_dataset(train, look_back)\n\u001b[0;32m      4\u001b[0m testX, testY \u001b[38;5;241m=\u001b[39m create_dataset(test, look_back)\n",
      "Cell \u001b[1;32mIn[96], line 4\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[1;34m(dataset, look_back)\u001b[0m\n\u001b[0;32m      2\u001b[0m dataX, dataY \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m-\u001b[39mlook_back\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     a \u001b[38;5;241m=\u001b[39m dataset[i:(i\u001b[38;5;241m+\u001b[39mlook_back), \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m     dataX\u001b[38;5;241m.\u001b[39mappend(a)\n\u001b[0;32m      6\u001b[0m     dataY\u001b[38;5;241m.\u001b[39mappend(dataset[i \u001b[38;5;241m+\u001b[39m look_back, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3804\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3809\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m   3810\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5925\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   5922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5923\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5924\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5925\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(0, 1, None), 0)"
     ]
    }
   ],
   "source": [
    "#Reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7e832",
   "metadata": {},
   "source": [
    "# Implement LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1599b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement LSTM model\n",
    "def create_dataset(X, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(X[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS = 10\n",
    "X_train, y_train = create_dataset(train_sentiment_scaled, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test_sentiment_scaled, TIME_STEPS)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model_lstm.add(LSTM(units=50))\n",
    "model_lstm.add(Dense(units=1))\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model_lstm.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "lstm_predictions_scaled = model_lstm.predict(X_test)\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
    "\n",
    "# Evaluate the models\n",
    "arima_rmse = np.sqrt(mean_squared_error(test_sentiment, arima_predictions))\n",
    "lstm_rmse = np.sqrt(mean_squared_error(test_sentiment, lstm_predictions))\n",
    "\n",
    "print(\"ARIMA RMSE:\", arima_rmse)\n",
    "print(\"LSTM RMSE:\", lstm_rmse)\n",
    "\n",
    "# Make predictions for 1 day, 3 days, and 7 days ahead\n",
    "def forecast_sentiment(model, data, steps):\n",
    "    last_window = data[-TIME_STEPS:]\n",
    "    forecast = []\n",
    "    for _ in range(steps):\n",
    "        prediction = model.predict(last_window.reshape(1, -1, 1))[0][0]\n",
    "        forecast.append(prediction)\n",
    "        last_window = np.roll(last_window, -1)\n",
    "        last_window[-1] = prediction\n",
    "    return forecast\n",
    "\n",
    "# Forecast sentiment using ARIMA\n",
    "arima_forecast_1day = model_arima_fit.forecast(steps=1)[0][0]\n",
    "arima_forecast_3day = model_arima_fit.forecast(steps=3)[0][-1]\n",
    "arima_forecast_7day = model_arima_fit.forecast(steps=7)[0][-1]\n",
    "\n",
    "# Forecast sentiment using LSTM\n",
    "lstm_forecast_1day = forecast_sentiment(model_lstm, test_sentiment_scaled[-TIME_STEPS:], 1)\n",
    "lstm_forecast_3day = forecast_sentiment(model_lstm, test_sentiment_scaled[-TIME_STEPS:], 3)[-1]\n",
    "lstm_forecast_7day = forecast_sentiment(model_lstm, test_sentiment_scaled[-TIME_STEPS:], 7)[-1]\n",
    "\n",
    "print(\"ARIMA 1-day forecast:\", arima_forecast_1day)\n",
    "print(\"ARIMA 3-day forecast:\", arima_forecast_3day)\n",
    "print(\"ARIMA 7-day forecast:\", arima_forecast_7day)\n",
    "print(\"LSTM 1-day forecast:\", lstm_forecast_1day)\n",
    "print(\"LSTM 3-day forecast:\", lstm_forecast_3day)\n",
    "print(\"LSTM 7-day forecast:\", lstm_forecast_7day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c675ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming df contains at least two columns: 'date' and 'sentiment'\n",
    "\n",
    "# Preprocess the data\n",
    "# Ensure 'date' column is in datetime format\n",
    "\n",
    "# Implement ARIMA model\n",
    "def train_arima_model(data):\n",
    "    model_arima = ARIMA(data, order=(5,1,0))\n",
    "    model_arima_fit = model_arima.fit(disp=0)\n",
    "    return model_arima_fit\n",
    "\n",
    "def forecast_arima(model, steps):\n",
    "    forecast = model.forecast(steps=steps)[0]\n",
    "    return forecast\n",
    "\n",
    "# Implement LSTM model\n",
    "def create_dataset(X, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(X[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def train_lstm_model(data, time_steps=10):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    X, y = create_dataset(data_scaled, time_steps)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "    model_lstm.add(LSTM(units=50))\n",
    "    model_lstm.add(Dense(units=1))\n",
    "\n",
    "    model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model_lstm.fit(X, y, epochs=100, batch_size=32, verbose=0)\n",
    "    \n",
    "    return model_lstm, scaler\n",
    "\n",
    "def forecast_lstm(model, scaler, data, steps, time_steps=10):\n",
    "    forecast = []\n",
    "    last_window = data[-time_steps:]\n",
    "    for _ in range(steps):\n",
    "        prediction = model.predict(last_window.reshape(1, -1, 1))[0][0]\n",
    "        forecast.append(prediction)\n",
    "        last_window = np.roll(last_window, -1)\n",
    "        last_window[-1] = prediction\n",
    "    forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n",
    "    return forecast\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define the layout of the dashboard\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id='forecast-graph'),\n",
    "    html.Label('Select Model:'),\n",
    "    dcc.Dropdown(\n",
    "        id='model-dropdown',\n",
    "        options=[\n",
    "            {'label': 'ARIMA', 'value': 'arima'},\n",
    "            {'label': 'LSTM', 'value': 'lstm'}\n",
    "        ],\n",
    "        value='arima'\n",
    "    ),\n",
    "    html.Label('Select Forecast Period:'),\n",
    "    dcc.Dropdown(\n",
    "        id='period-dropdown',\n",
    "        options=[\n",
    "            {'label': '1 Day', 'value': 1},\n",
    "            {'label': '3 Days', 'value': 3},\n",
    "            {'label': '7 Days', 'value': 7}\n",
    "        ],\n",
    "        value=1\n",
    "    )\n",
    "])\n",
    "\n",
    "# Define callback to update the graph based on user input\n",
    "@app.callback(\n",
    "    Output('forecast-graph', 'figure'),\n",
    "    [Input('model-dropdown', 'value'),\n",
    "     Input('period-dropdown', 'value')]\n",
    ")\n",
    "def update_graph(selected_model, forecast_period):\n",
    "    if selected_model == 'arima':\n",
    "        model = train_arima_model(df['sentiment'])\n",
    "        forecast = forecast_arima(model, forecast_period)\n",
    "    elif selected_model == 'lstm':\n",
    "        lstm_model, scaler = train_lstm_model(df['sentiment'])\n",
    "        forecast = forecast_lstm(lstm_model, scaler, df['sentiment'], forecast_period)\n",
    "    \n",
    "    # Generate x-axis values (dates)\n",
    "    dates = pd.date_range(start=df['date'].iloc[-1], periods=forecast_period + 1)[1:]\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = {\n",
    "        'data': [\n",
    "            {'x': dates, 'y': forecast, 'type': 'line', 'name': 'Forecast'}\n",
    "        ],\n",
    "        'layout': {\n",
    "            'title': f'{selected_model.upper()} Forecast for {forecast_period} Days',\n",
    "            'xaxis': {'title': 'Date'},\n",
    "            'yaxis': {'title': 'Sentiment'}\n",
    "        }\n",
    "    }\n",
    "    return fig\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fd437",
   "metadata": {},
   "source": [
    "# TIME SERIES FORECASTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ead0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
